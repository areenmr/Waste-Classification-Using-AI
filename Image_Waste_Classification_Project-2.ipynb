{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0kFXTnyW0bN",
        "outputId": "d3c81c14-c19a-4602-dc6d-b096a6fc127b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install & Import Libraries\n",
        "!pip install tensorflow matplotlib opendatasets\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import opendatasets as od\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYtjyX2fXHuu",
        "outputId": "a7a084cb-99d5-4b7b-a99d-6beca2ed141e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: layalalsultan\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\n",
            "Downloading garbage-classification.zip to ./garbage-classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.0M/82.0M [00:00<00:00, 2.70GB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['paper', 'glass', 'trash', 'plastic', 'cardboard', 'metal']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Download Dataset from Kaggle\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\")\n",
        "\n",
        "# Adjust dataset path\n",
        "dataset_path = \"/content/garbage-classification/Garbage classification/Garbage classification\"\n",
        "print(os.listdir(dataset_path))  # will show folders like cardboard, glass, etc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j4YfDHlLXVA6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load & Preprocess Images\n",
        "\n",
        "image_size = 224\n",
        "data, labels = [], []\n",
        "\n",
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_path = os.path.join(dataset_path, class_name)\n",
        "    for img_name in os.listdir(class_path):\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (image_size, image_size))\n",
        "            img = img / 255.0\n",
        "            data.append(img)\n",
        "            labels.append(class_map[class_name])\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "699koRspXbRz"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  Split Dataset\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qF5hjYQoXcp4"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWOHZQLFXfqp",
        "outputId": "10cc2c06-d96a-44ec-e1f8-7506f48dd45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/waste_model_v2.h5\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model (Using Transfer Learning)\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/waste_model_v2.h5\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\" نموذج محفوظ موجود بالفعل، سيتم تحميله\")\n",
        "    model = load_model(model_path)\n",
        "    print(\" تم تحميل النموذج بنجاح، لا حاجة لإعادة التدريب\")\n",
        "else:\n",
        "    print(\"(Transfer Learningباستخدام ) لا يوجد نموذج محفوظ، سيتم البدء في تدريب جديد\")\n",
        "\n",
        "    # 1. Load the previously trained base model (MobileNetV2)\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=(image_size, image_size, 3),\n",
        "        include_top=False,  # We do not want to include the final ImageNet classifier layer\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # 2. Freeze the base model\n",
        "    base_model.trainable = False # We don't want to re train the layers that already learned the features\n",
        "\n",
        "    # 3. Adding a classifier \"head\"\n",
        "    # We implemented the Keras functional API\n",
        "    inputs = Input(shape=(image_size, image_size, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    outputs = Dense(6, activation='softmax')(x) # 6 classes\n",
        "\n",
        "    # 4. Create new model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 5. Defining when to stop\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5,             # Stop after 5 epochs of no improvement\n",
        "        restore_best_weights=True # Keep the best version of the model\n",
        "    )\n",
        "    # If the model gets \"stuck\", reduce the learning rate\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "    callbacks_list = [early_stop, reduce_lr]\n",
        "\n",
        "    # 6. Train model\n",
        "    history = model.fit(\n",
        "        train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "        epochs=50,  # Increased epochs with the help of EarlyStopping to find the best one\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=callbacks_list # Use the callbacks from step 5\n",
        "    )\n",
        "\n",
        "    # 7. Save model in google drive to prevent re training every time\n",
        "    model.save(model_path)\n",
        "    print(\" تم حفظ النموذج في Google Drive للاستخدام لاحقاً\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFj6hNQj74UB",
        "outputId": "f627cf24-b6b6-4aa8-a776-1fafc72982b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " نموذج محفوظ موجود بالفعل، سيتم تحميله\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " تم تحميل النموذج بنجاح، لا حاجة لإعادة التدريب\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XGKNE5TDYAHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc9e4d5-6b30-48d5-f808-ed76dac44ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 957ms/step - accuracy: 0.9434 - loss: 0.2108\n",
            " دقة النموذج: 93.42%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate Model\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\" دقة النموذج: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4qf2D_eJYFrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513f7493-bb8e-4fe2-e75a-b01fb988eccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step\n",
            " الفئة المتوقعة: زجاج (glass)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Predict with Arabic Labels\n",
        "\n",
        "arabic_labels = {\n",
        "    'cardboard': 'كرتون',\n",
        "    'glass': 'زجاج',\n",
        "    'metal': 'معدن',\n",
        "    'paper': 'ورق',\n",
        "    'plastic': 'بلاستيك',\n",
        "    'trash': 'نفايات'\n",
        "}\n",
        "\n",
        "def predict_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\" خطأ: تعذر تحميل الصورة من {img_path}\")\n",
        "        return\n",
        "    img = cv2.resize(img, (image_size, image_size))\n",
        "    img = np.expand_dims(img, axis=0) / 255.0\n",
        "    prediction = model.predict(img)\n",
        "    class_idx = np.argmax(prediction)\n",
        "    class_name = class_names[class_idx]\n",
        "    print(f\" الفئة المتوقعة: {arabic_labels[class_name]} ({class_name})\")\n",
        "\n",
        "# Example usage:\n",
        "predict_image(\"/content/garbage-classification/Garbage classification/Garbage classification/glass/glass1.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m8yGKjgiYmr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "9b2e88ab-bfc0-4726-9287-945d4edc0c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3a3219790ec5faad8f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3a3219790ec5faad8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def classify_image_text_only(image):\n",
        "    import cv2, numpy as np\n",
        "    img = cv2.resize(image, (image_size, image_size))\n",
        "    img = np.expand_dims(img, axis=0) / 255.0\n",
        "    prediction = model.predict(img)\n",
        "    class_idx = np.argmax(prediction)\n",
        "    class_name = class_names[class_idx]\n",
        "    arabic_label = arabic_labels[class_name]\n",
        "    return f\" الفئة المتوقعة: {arabic_label}\"\n",
        "\n",
        "interface_text = gr.Interface(\n",
        "    fn=classify_image_text_only,\n",
        "    inputs=gr.Image(type=\"numpy\", label=\" حمّل صورة النفايات\"),\n",
        "    outputs=gr.Textbox(label=\"النتيجة بالعربية\"),\n",
        "    title=\"نظام تصنيف النفايات بالصور\",\n",
        "    description=\"اختر صورة نفايات لمعرفة نوعها (ورق، بلاستيك، زجاج، كرتون، معدن، نفايات).\"\n",
        ")\n",
        "\n",
        "interface_text.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}